{
    "class_name": "Tokenizer",
    "config": {
        "num_words": null,
        "filters": "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n",
        "lower": true,
        "split": " ",
        "char_level": false,
        "oov_token": null,
        "document_count": 2,
        "word_counts": "{\"correct\": 1, \"wrong\": 1}",
        "word_docs": "{\"correct\": 1, \"wrong\": 1}",
        "index_docs": "{\"1\": 1, \"2\": 1}",
        "index_word": "{\"1\": \"correct\", \"2\": \"wrong\"}",
        "word_index": "{\"correct\": 1, \"wrong\": 2}"
    }
}